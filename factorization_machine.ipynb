{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Factorization Machine**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Outline:\n",
    "\n",
    "1. Background\n",
    "2. Simplified Workflows.\n",
    "3. Importing Data\n",
    "4. Data Preparations\n",
    "5. Data Preprocessing\n",
    "6. Modeling\n",
    "7. Hyperparameter Tuning\n",
    "8. Evaluation\n",
    "9. Decision Process (Recommendation Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Background**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A Travel Company **Trip Advisor** are having a problem with its user retention.\n",
    "- In 3 months, the user retention rate dropped almost 20% which really affects **Trip Advisor** revenues.\n",
    "- After doing an urgent user research, **Trip Advisor** team found that user spend time less in their platform because not knowing what to do in the platform ,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Objective\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our business objective would be **increasing user retention** to **20%** (assumed ofcourse) in 3 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can create a **travel recommendation** to help **users browse** the hotel / venue for their vacation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal, of our recommendation is to recommend movies hotel / venue they might like, however its quite abstract. We may have to create **proxy label** to approach that problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To approach those, some appropriate proxy labels are :    \n",
    "- Scale of rating (star) user  given to a hotel / venue\n",
    "- User click the movie\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have data of user rating towards item , in this case hotel / venue, beside we also have **contextual data**.\n",
    "\n",
    "We can move further into machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our task** is to predict number of stars given from user to a movie.\n",
    "\n",
    "With  stars itself is in continous value, hence we can conclude it as **regression task**.\n",
    "\n",
    "We now have a clearer picture what we should do, However we need more precise solution in recommender system context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach in Personalized Recommender System can be divided based on the presence of interaction data (explicit / implicit) data:     \n",
    "\n",
    "1. When the interaction data is not exists, the solution that can be implemented is using content feature, **Content Based** Filtering\n",
    "\n",
    "2. When the interaction data is exists, we can use **Collaborative** Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use **Context Aware Recommender System** to utilize context data in our recommender system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Metrics\n",
    "---\n",
    "We have already established some points :\n",
    "- Our task is to predict stars that will be given by users to certain movies\n",
    "- We will use Collaborative Filtering approach\n",
    "\n",
    "Regarding those, we need to measure the success of our model ( metrics), based on the points mentioned, our goal is to predict as close as possible the predicted rating to user true rating,\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to minimize $(\\text{True Rating - Predicted Rating})$, some choices of appropriate metrics are :     \n",
    "- Mean Absolute Error\n",
    "- Mean Square Error\n",
    "- Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to its `differentiable` property , we will choose **MSE/RMSE** as our model metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Description\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data is obtained from [Trip Advisor dataset](https://grouplens.org/datasets/movielens/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one file we use\n",
    "\n",
    "**rating data** : `tripadvisor_travel.csv`\n",
    "\n",
    "<center>\n",
    "\n",
    "|Features|Descriptions|Data Type|\n",
    "|:--|:--|:--:|\n",
    "|`user_id`|The user ID|`object`|\n",
    "|`item_id`|The item / hotel ID|`int`|\n",
    "|`rating`|Rating given from user to item. Ranging from `1` to `5`|`float`|\n",
    "|`trip`|Trip Type (Context Data) : `['SOLO', 'FAMILY', 'COUPLES', 'BUSINESS', 'FRIENDS']`|`object`|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Recommender System Workflow** (Simplified)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='cyan'>1. Importing Data</font>\n",
    "\n",
    "```\n",
    "1. Load the data.\n",
    "2. Check the shape & type of data.\n",
    "3. Handle the duplicates data to maintain data validity.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='cyan'>2.Modelling : Factorization Machine</font>\n",
    "\n",
    "```\n",
    "1. Creating Utility Matrix\n",
    "2. Training + Model Selection  :     \n",
    "    - Baseline (Dummy Classifier)\n",
    "    - Factorization Machines\n",
    "\n",
    "4. Evaluating Model\n",
    "  - Rating Prediction Task\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='cyan'>3. Generating Recommendation / Predictions</font>\n",
    "\n",
    "```\n",
    "1. Predict recommendation of user-i to unrated item-j with context\n",
    "2. Predict recommendation of user-i to all their unrated items with context\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Importing Data**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we do?\n",
    "1. Load the data.\n",
    "2. Check the shape & type of data.\n",
    "3. Handle the duplicates data to maintain data validity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load this library\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5C28F393B23BB894523AE7126A7AE445</td>\n",
       "      <td>219668</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>223860</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B99CFBB5411EDC8881D13B7A4B313ADA</td>\n",
       "      <td>75680</td>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>224783</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7CEFF5C32BA1F3B186E7838C7D3FE25E</td>\n",
       "      <td>222984</td>\n",
       "      <td>5</td>\n",
       "      <td>COUPLES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  item_id  rating     trip\n",
       "0  5C28F393B23BB894523AE7126A7AE445   219668       5     SOLO\n",
       "1  3FA27F6E8AC712A82C69C4EDD8B912CC   223860       5     SOLO\n",
       "2  B99CFBB5411EDC8881D13B7A4B313ADA    75680       5   FAMILY\n",
       "3  3FA27F6E8AC712A82C69C4EDD8B912CC   224783       5     SOLO\n",
       "4  7CEFF5C32BA1F3B186E7838C7D3FE25E   222984       5  COUPLES"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Giving the path\n",
    "trip_data_path = r\"data\\trip_advisor_travel.csv\"\n",
    "\n",
    "# Loading the data\n",
    "data = pd.read_csv(trip_data_path,\n",
    "                          delimiter = ',')\n",
    "\n",
    "# Showing first 5 data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14175, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data shapes\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id    object\n",
       "item_id     int64\n",
       "rating      int64\n",
       "trip       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Features type: `user_id`, `item_id`, `rating`, and `trip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicate data\n",
    "data.duplicated(subset=['user_id', 'item_id','trip']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.drop_duplicates(subset=['user_id', 'item_id','trip'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check duplicate data\n",
    "clean_data.duplicated(subset=['user_id', 'item_id','trip']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Data Shape before dropping duplicate data (14175, 4)\n",
      "- Data Shape after dropping duplicate data (13849, 4)\n"
     ]
    }
   ],
   "source": [
    "print('- Data Shape before dropping duplicate data', data.shape)\n",
    "print('- Data Shape after dropping duplicate data', clean_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating loading fucntion\n",
    "def load_trip_data(path):\n",
    "    \"\"\"\n",
    "    Function to load data & remove from duplicates\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        The path of trip data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trip_data : pandas DataFrame\n",
    "        The sample of trip_data data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    trip_data = pd.read_csv(path, delimiter=',')\n",
    "    print('- Original data shape :', trip_data.shape)\n",
    "\n",
    "    # drop duplicate data\n",
    "    trip_data = trip_data.drop_duplicates(subset=['user_id', 'item_id','trip'])\n",
    "    print('- After dropping duplicate data shape : ',trip_data.shape)\n",
    "\n",
    "    return trip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original data shape : (14175, 4)\n",
      "- After dropping duplicate data shape :  (13849, 4)\n"
     ]
    }
   ],
   "source": [
    "# Loading rating data\n",
    "trip_data = load_trip_data(path = trip_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5C28F393B23BB894523AE7126A7AE445</td>\n",
       "      <td>219668</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>223860</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B99CFBB5411EDC8881D13B7A4B313ADA</td>\n",
       "      <td>75680</td>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>224783</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7CEFF5C32BA1F3B186E7838C7D3FE25E</td>\n",
       "      <td>222984</td>\n",
       "      <td>5</td>\n",
       "      <td>COUPLES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  item_id  rating     trip\n",
       "0  5C28F393B23BB894523AE7126A7AE445   219668       5     SOLO\n",
       "1  3FA27F6E8AC712A82C69C4EDD8B912CC   223860       5     SOLO\n",
       "2  B99CFBB5411EDC8881D13B7A4B313ADA    75680       5   FAMILY\n",
       "3  3FA27F6E8AC712A82C69C4EDD8B912CC   224783       5     SOLO\n",
       "4  7CEFF5C32BA1F3B186E7838C7D3FE25E   222984       5  COUPLES"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Modelling**: Context Aware Collaborative Filtering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we createdrecommender system by Matrix Factorization / Latent Factor Model Approach. However, in our matrix factorization approach we have not addressed how to add contextual features, such as : time, place, etc.\n",
    "\n",
    "We might think that now, our utility matrix is not only involved user factor or item factor, but also add **context** factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learnt that, there are 3 solutions to add context into our modelling :      \n",
    "1. Contextual PreFiltering\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/258920113/figure/fig2/AS:297180343619593@1447864723798/Flow-model-of-contextual-pre-filtering-6.png\" img>\n",
    "\n",
    "<a href=\"https://www.researchgate.net/publication/258920113/figure/fig2/AS:297180343619593@1447864723798/Flow-model-of-contextual-pre-filtering-6.png\">Source </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Contextual PostFiltering\n",
    "\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/304781330/figure/fig1/AS:381298381213699@1467920026542/Contextual-post-filtering.png\" img>\n",
    "\n",
    "<a href=\"https://www.researchgate.net/publication/304781330/figure/fig1/AS:381298381213699@1467920026542/Contextual-post-filtering.png\">Source </a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Contextual Modelling\n",
    "\n",
    "In this approach we do'not filter context, instead we use it as \"feature\" in our modelling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some approach :    \n",
    "1. Latent Factor Approach\n",
    "\n",
    "  example : Tensor Factorization, Matrix Factorization with Side Features\n",
    "\n",
    "2. Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a personalized RecSys, we can follow these steps:\n",
    "\n",
    "```\n",
    "1. Data Preparation --> Create utility matrix & Split Train-Test\n",
    "2. Train recommendation model --> Baseline, User to User CF (KNN) & Item to Item CF (KNN)\n",
    "3. Choosing Best Model\n",
    "5. Evaluate Final Model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Model From Scratch\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine from Scratch : Regression Task\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task : Rating Prediction**\n",
    "\n",
    "Factorization Machine is basically the same as our `familiar` Linear Regression model basic form, which is :\n",
    "\n",
    "$$\n",
    "f(x) = mx + c\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have lot of feature it will become :\n",
    "$$\n",
    "f(x) = m_1.x_1 + ... m_n.x_n + c \n",
    "$$\n",
    "\n",
    "and to summarize :\n",
    "$$\n",
    "f(x) = \\sum_i^nm_i.x_i + c \n",
    "$$\n",
    "\n",
    "where $m$ is weight or coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where in Recommender System context we hypothesize that our features such as :    \n",
    "1. User\n",
    "2. Item\n",
    "3. Contextual data\n",
    "\n",
    "has relation to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add interaction factor to each feature as follow :\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i^nm_i.x_i + c + \\sum_{i=1} \\sum_{j=1+1} w_{i,j} x_i.x_j \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "instead of using weight to each interaction, we can create interaction term for each feature and represented it as latent factor with below equation :\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i^nm_i.x_i + c + \\sum_{i=1} \\sum_{j=1+1} <v_i.v_j> x_i.x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hereby we represent our feature as latent factor, hence we can choose number of number latent factor to represent each feature, let say if we choose 3 latent factor.\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i^nm_i.x_i + c + \\sum_{f=1}^{k=3} \\sum_{i=1} \\sum_{j=1+1} v_{i,f}.v_{i,f} x_i.x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in general we depend on number of latent factor `k`\n",
    "\n",
    "$$\n",
    "f(x) = \\sum_i^nm_i.x_i + c + \\sum_{f=1}^{k} \\sum_{i=1} \\sum_{j=1+1} v_{i,f}.v_{i,f} x_i.x_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation\n",
    "\n",
    "In this process we will prepare our data so that we can continue to training process, the expected output should be array with size `<n_records x features>` with\n",
    "\n",
    "with features ordered :     \n",
    "- feature 1 : `userId`\n",
    "- feature 2 : `movieId`\n",
    "- feature 3 : `rating`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=1Qqp4HD9qVkiwKxn2clVtfdpujSd6kUea\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping User and Item ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create mapping, we can simply create dictionary, which stores :    \n",
    "- UserId to ordered id\n",
    "```python\n",
    "user_to_id = {userid : orderedid}\n",
    "#example\n",
    "user_to_id = {'ab' : 1}\n",
    "```\n",
    "- Ordered id to UserId\n",
    "```python\n",
    "user_to_id = {orderedid : userid }\n",
    "#example\n",
    "user_to_id = {1 : 'ab' }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping user to ordered id\n",
    "user_to_id = {user : idx for idx,user in enumerate(trip_data['user_id'].unique())}\n",
    "\n",
    "id_to_user = {idx : user for idx,user in enumerate(trip_data['user_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping item to ordered id\n",
    "item_to_id = {item : idx for idx,item in enumerate(trip_data['item_id'].unique())}\n",
    "\n",
    "id_to_item = {idx : item for idx,item in enumerate(trip_data['item_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data = trip_data.copy()\n",
    "mapped_data['user_id'] = mapped_data['user_id'].map(user_to_id)\n",
    "mapped_data['item_id'] = mapped_data['item_id'].map(item_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5C28F393B23BB894523AE7126A7AE445</td>\n",
       "      <td>219668</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>223860</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B99CFBB5411EDC8881D13B7A4B313ADA</td>\n",
       "      <td>75680</td>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3FA27F6E8AC712A82C69C4EDD8B912CC</td>\n",
       "      <td>224783</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7CEFF5C32BA1F3B186E7838C7D3FE25E</td>\n",
       "      <td>222984</td>\n",
       "      <td>5</td>\n",
       "      <td>COUPLES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id  item_id  rating     trip\n",
       "0  5C28F393B23BB894523AE7126A7AE445   219668       5     SOLO\n",
       "1  3FA27F6E8AC712A82C69C4EDD8B912CC   223860       5     SOLO\n",
       "2  B99CFBB5411EDC8881D13B7A4B313ADA    75680       5   FAMILY\n",
       "3  3FA27F6E8AC712A82C69C4EDD8B912CC   224783       5     SOLO\n",
       "4  7CEFF5C32BA1F3B186E7838C7D3FE25E   222984       5  COUPLES"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>trip</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>SOLO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>COUPLES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating     trip\n",
       "0        0        0       5     SOLO\n",
       "1        1        1       5     SOLO\n",
       "2        2        2       5   FAMILY\n",
       "3        1        3       5     SOLO\n",
       "4        3        4       5  COUPLES"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that, now beside our utility matrix we have `context` data , in our case, `trip` type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Into : \n",
    "     \n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1pAkYN3kJ-O7-p5BLEwucRhjKEr7wxToO\" width=600>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting data into categorical type, so that we could create One Hot Encoding based on available data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_data['user_id'] = mapped_data['user_id'].astype('category')\n",
    "mapped_data['item_id'] = mapped_data['item_id'].astype('category')\n",
    "encoded_data = pd.get_dummies(mapped_data[['user_id','item_id','trip']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_0</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_2264</th>\n",
       "      <th>item_id_2265</th>\n",
       "      <th>item_id_2266</th>\n",
       "      <th>item_id_2267</th>\n",
       "      <th>item_id_2268</th>\n",
       "      <th>trip_BUSINESS</th>\n",
       "      <th>trip_COUPLES</th>\n",
       "      <th>trip_FAMILY</th>\n",
       "      <th>trip_FRIENDS</th>\n",
       "      <th>trip_SOLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_0  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5   \n",
       "0       True      False      False      False      False      False  \\\n",
       "1      False       True      False      False      False      False   \n",
       "2      False      False       True      False      False      False   \n",
       "3      False       True      False      False      False      False   \n",
       "4      False      False      False       True      False      False   \n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  item_id_2264   \n",
       "0      False      False      False      False  ...         False  \\\n",
       "1      False      False      False      False  ...         False   \n",
       "2      False      False      False      False  ...         False   \n",
       "3      False      False      False      False  ...         False   \n",
       "4      False      False      False      False  ...         False   \n",
       "\n",
       "   item_id_2265  item_id_2266  item_id_2267  item_id_2268  trip_BUSINESS   \n",
       "0         False         False         False         False          False  \\\n",
       "1         False         False         False         False          False   \n",
       "2         False         False         False         False          False   \n",
       "3         False         False         False         False          False   \n",
       "4         False         False         False         False          False   \n",
       "\n",
       "   trip_COUPLES  trip_FAMILY  trip_FRIENDS  trip_SOLO  \n",
       "0         False        False         False       True  \n",
       "1         False        False         False       True  \n",
       "2         False         True         False      False  \n",
       "3         False        False         False       True  \n",
       "4          True        False         False      False  \n",
       "\n",
       "[5 rows x 4645 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen that we already prepare all the data, we may want to use the preprocessing step again, we better create the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map data\n",
    "def map_data(trip_data) :\n",
    "    user_to_id = {user : idx for idx,user in enumerate(trip_data['user_id'].unique())}\n",
    "    id_to_user = {idx : user for idx,user in enumerate(trip_data['user_id'].unique())}\n",
    "\n",
    "    # mapping item to ordered id\n",
    "    item_to_id = {item : idx for idx,item in enumerate(trip_data['item_id'].unique())}\n",
    "    id_to_item = {idx : item for idx,item in enumerate(trip_data['item_id'].unique())}\n",
    "\n",
    "    trip_data['user_id'] = trip_data['user_id'].map(user_to_id)\n",
    "    trip_data['item_id'] = trip_data['item_id'].map(item_to_id)\n",
    "\n",
    "    user_mapping = {\n",
    "        'user_to_id' : user_to_id,\n",
    "        'id_to_user' : id_to_user\n",
    "    }\n",
    "\n",
    "    item_mapping = {\n",
    "        'item_to_id' : item_to_id,\n",
    "        'id_to_item' : id_to_item\n",
    "    }\n",
    "\n",
    "    return trip_data,user_mapping,item_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(trip_data) :\n",
    "    \"\"\"\n",
    "    Function to preprocess data\n",
    "    Parameters\n",
    "    ----------\n",
    "    trip_data : pandas.DataFrame\n",
    "        contain the trip data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    trip_data : pandas DataFrame\n",
    "        The sample of trip_data data\n",
    "\n",
    "    mapping_user : dict\n",
    "        contain mappers :\n",
    "        1. user to id\n",
    "        2. id to user\n",
    "\n",
    "    mapping_item : dict\n",
    "        contain mappers :\n",
    "        1. item to id\n",
    "        2. id to item\n",
    "\n",
    "    \"\"\"\n",
    "    # copy to avoid overwriting the original dataframe\n",
    "    trip_data = trip_data.copy()\n",
    "\n",
    "    trip_data, mapping_user, mapping_item = map_data(trip_data= trip_data)\n",
    "\n",
    "    # change data type from user and item id to category\n",
    "    trip_data['user_id'] = trip_data['user_id'].astype('category')\n",
    "    trip_data['item_id'] = trip_data['item_id'].astype('category')\n",
    "\n",
    "    # create one hot encoding\n",
    "    encoded_data = pd.get_dummies(trip_data[['user_id','item_id','trip']], dtype=int)\n",
    "\n",
    "    # combine again with the target variable\n",
    "    all_data = pd.concat([encoded_data,trip_data[['rating']]],axis=1)\n",
    "\n",
    "\n",
    "    feature_names = encoded_data.columns.tolist()\n",
    "    return all_data, mapping_user, mapping_item,feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data,mapping_user,mapping_item,feature_names = preprocess_data(trip_data=trip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_0</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_2265</th>\n",
       "      <th>item_id_2266</th>\n",
       "      <th>item_id_2267</th>\n",
       "      <th>item_id_2268</th>\n",
       "      <th>trip_BUSINESS</th>\n",
       "      <th>trip_COUPLES</th>\n",
       "      <th>trip_FAMILY</th>\n",
       "      <th>trip_FRIENDS</th>\n",
       "      <th>trip_SOLO</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4646 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_0  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5   \n",
       "0          1          0          0          0          0          0  \\\n",
       "1          0          1          0          0          0          0   \n",
       "2          0          0          1          0          0          0   \n",
       "3          0          1          0          0          0          0   \n",
       "4          0          0          0          1          0          0   \n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  item_id_2265   \n",
       "0          0          0          0          0  ...             0  \\\n",
       "1          0          0          0          0  ...             0   \n",
       "2          0          0          0          0  ...             0   \n",
       "3          0          0          0          0  ...             0   \n",
       "4          0          0          0          0  ...             0   \n",
       "\n",
       "   item_id_2266  item_id_2267  item_id_2268  trip_BUSINESS  trip_COUPLES   \n",
       "0             0             0             0              0             0  \\\n",
       "1             0             0             0              0             0   \n",
       "2             0             0             0              0             0   \n",
       "3             0             0             0              0             0   \n",
       "4             0             0             0              0             1   \n",
       "\n",
       "   trip_FAMILY  trip_FRIENDS  trip_SOLO  rating  \n",
       "0            0             0          1       5  \n",
       "1            0             0          1       5  \n",
       "2            1             0          0       5  \n",
       "3            0             0          1       5  \n",
       "4            0             0          0       5  \n",
       "\n",
       "[5 rows x 4646 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we already satistified with `Factorization Machines` requirement, however we see that we have `4646` features with mostly zero values. It will be quite inefficient to store if we store it as dataframe , hence we will convert it to sparse matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Converting DataFrame to sparse matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Splitting\n",
    "\n",
    "We are going to split our data into training and test data, we are not splitting like before, because our utility matrix + context now become general machine learning algorithm training samples `<X,Y>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = processed_data.drop('rating',axis=1)\n",
    "y = processed_data['rating']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data\n",
    "X_train,X_test,y_train,y_test = train_test_split(X.to_numpy(), y.to_numpy(), test_size= 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train (11079, 4645)\n",
      "Shape of X_train (2770, 4645)\n",
      "Shape of y_train (11079,)\n",
      "Shape of y_test (2770,)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of X_train',X_train.shape)\n",
    "print('Shape of X_train',X_test.shape)\n",
    "print('Shape of y_train',y_train.shape)\n",
    "print('Shape of y_test',y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### All about Sparse Matrix\n",
    "\n",
    "\n",
    "We previously has formed , `Sparse Data`, mostly with 0 values, however it will deal with serious problem, if we do that in just pure numpy array. We need to utilize what called as **sparse matrix**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of variety sparse matrix, but most common is `Compressed Sparse Row/Column`\n",
    "\n",
    "<img src=\"https://matteding.github.io/images/csr.gif\" >\n",
    "<center> <a href=\"https://matteding.github.io/images/csr.gif\">Source</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSR matrix consist of several elements :      \n",
    "1. Indices\n",
    "\n",
    "   Could be column or row location which contain data,\n",
    "   if we use compressed sparse row --> our indices is the column\n",
    "\n",
    "2. Indices Pointer\n",
    "\n",
    "   This is the opposite of our indices, if our indices is in column , then the pointer is in row,\n",
    "   \n",
    "\n",
    "3. Data\n",
    "\n",
    "   Non Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's say we have a sparse matrix as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8, 0, 2, 0, 0],\n",
       "       [0, 0, 5, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 7, 1, 2],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 9, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_matrix = np.array(\n",
    "        [[8, 0, 2, 0, 0],\n",
    "        [0, 0, 5, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 7, 1, 2],\n",
    "        [0, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 9, 0]])\n",
    "\n",
    "sparse_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we could built the sparse matrix version of it as below :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7x5 sparse matrix of type '<class 'numpy.intc'>'\n",
       "\twith 7 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import scipy\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# convert to csr_matrix\n",
    "s_matrix_csr = sp.csr_matrix(sparse_matrix)\n",
    "s_matrix_csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Numpy Version of Sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row : 0 Col : 0 Value :  8\n",
      "Row : 0 Col : 1 Value :  0\n",
      "Row : 0 Col : 2 Value :  2\n",
      "Row : 0 Col : 3 Value :  0\n",
      "Row : 0 Col : 4 Value :  0\n",
      "Row : 1 Col : 0 Value :  0\n",
      "Row : 1 Col : 1 Value :  0\n",
      "Row : 1 Col : 2 Value :  5\n",
      "Row : 1 Col : 3 Value :  0\n",
      "Row : 1 Col : 4 Value :  0\n",
      "Row : 2 Col : 0 Value :  0\n",
      "Row : 2 Col : 1 Value :  0\n",
      "Row : 2 Col : 2 Value :  0\n",
      "Row : 2 Col : 3 Value :  0\n",
      "Row : 2 Col : 4 Value :  0\n",
      "Row : 3 Col : 0 Value :  0\n",
      "Row : 3 Col : 1 Value :  0\n",
      "Row : 3 Col : 2 Value :  0\n",
      "Row : 3 Col : 3 Value :  0\n",
      "Row : 3 Col : 4 Value :  0\n",
      "Row : 4 Col : 0 Value :  0\n",
      "Row : 4 Col : 1 Value :  0\n",
      "Row : 4 Col : 2 Value :  7\n",
      "Row : 4 Col : 3 Value :  1\n",
      "Row : 4 Col : 4 Value :  2\n",
      "Row : 5 Col : 0 Value :  0\n",
      "Row : 5 Col : 1 Value :  0\n",
      "Row : 5 Col : 2 Value :  0\n",
      "Row : 5 Col : 3 Value :  0\n",
      "Row : 5 Col : 4 Value :  0\n",
      "Row : 6 Col : 0 Value :  0\n",
      "Row : 6 Col : 1 Value :  0\n",
      "Row : 6 Col : 2 Value :  0\n",
      "Row : 6 Col : 3 Value :  9\n",
      "Row : 6 Col : 4 Value :  0\n"
     ]
    }
   ],
   "source": [
    "# loop all over row\n",
    "  # loop all over col\n",
    "for row_idx in range(sparse_matrix.shape[0]) :\n",
    "    for col_idx in range(sparse_matrix.shape[1]) :\n",
    "        print(f'Row : {row_idx} Col : {col_idx} Value : ',sparse_matrix[row_idx,col_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compressed Sparse Row** version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this version of sparse matrix we can only access **Non Zero Values** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 3, 3, 3, 6, 6, 7])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_matrix_csr.indptr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, 2, 3, 4, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_matrix_csr.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data 8\n",
      "data 2\n",
      "data 5\n",
      "data 7\n",
      "data 1\n",
      "data 2\n",
      "data 9\n"
     ]
    }
   ],
   "source": [
    "# we can loop all over indices pointer\n",
    "row_length = sparse_matrix.shape[0]\n",
    "for i in range(row_length)  :\n",
    "    # access indices pointer, we should add +1\n",
    "    for index in range(s_matrix_csr.indptr[i],s_matrix_csr.indptr[i+1]) :\n",
    "      # print('index',index)\n",
    "      print('data',s_matrix_csr.data[index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to scipy.csr matrix\n",
    "X_train_sparse = sp.csr_matrix(X_train)\n",
    "X_test_sparse  = sp.csr_matrix(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "\n",
    "$$\n",
    "\\text{Objective}= \\underset{}{\\min}\n",
    "\\left [\n",
    " \\sum_{n \\in D} (y^{(n)}-\\hat{y}^{(n)})^2\n",
    "\\right ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}^{(n)}\n",
    "&= w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i}^{(n)} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 {x_{i}^{(n)}}^2 \\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "Our main goal is to minimize prediction error, between true rating and predicted rating\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add term $\\cfrac{1}{2}$ (optional), the purpose is to make the derivative more simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{Objective}= \\underset{}{\\min}\n",
    "\\left [\n",
    "\\cfrac{1}{2} \\sum_{n \\in D} (y^{(n)}-\\hat{y}^{(n)})^2\n",
    "\\right ]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Objective}\n",
    "&= \\underset{w_0,w^*,v*}{min} \\left [\\cfrac{1}{2}\\sum_{n \\in D} \\left ( y^{n}- ( w_0 + \\sum_{i=1} w_i.x_i^{(n)} +  \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_i^{(n)} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 {x_i^{(n)}}^2 \\right) \\right )^2 \\right ]\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Regularization\n",
    "\n",
    "the purpose of adding regularization is to make our model can avoid overfitting, we can add Ridge Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Objective Function**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Objective}\n",
    "&= \\underset{w_0,w^*,v*}{min} \\left [\\cfrac{1}{2}\\sum_{n \\in D} \\left ( y^{n}- ( w_0 + \\sum_{i=1} w_i.x_i^{(n)} +  \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_i^{(n)} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 {x_i^{(n)}}^2 \\right) \\right )^2 \\right ] + \\cfrac{\\lambda}{2} \\left [\\sum_{i}(w_i^2 +||v_i||^2 ) \\right ]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization\n",
    "\n",
    "There are three parameters of Factorization Machines :   \n",
    "- $w_0$ : intercept a scalar value\n",
    "- $w_i$ : feature weight , vector with size of `<n_features x 1>`\n",
    "- $v_i$ : feature interaction latent factor, matrix with size of `<n_features x n_factor>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample,n_features = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of latent factor\n",
    "n_factor = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "np.random.seed(49)\n",
    "\n",
    "# initialize parameter\n",
    "# intercept\n",
    "intercept = 0\n",
    "\n",
    "# feature weight\n",
    "w = np.random.normal(size=(n_features,1))\n",
    "\n",
    "# feature interaction weight\n",
    "v = np.random.normal(size=(n_factor,n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of feature weight (4645, 1)\n",
      "Shape of feature interaction weight (50, 4645)\n"
     ]
    }
   ],
   "source": [
    "# print shape\n",
    "print('Shape of feature weight',w.shape)\n",
    "print('Shape of feature interaction weight',v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def mse_loss(y_true, y_pred,lambda_coef,lambda_factor,w,v):\n",
    "    \"\"\"\n",
    "    MSE Loss plus regularization term\n",
    "    \"\"\"\n",
    "    loss = ((y_true - y_pred)**2) / 2\n",
    "    regularization_loss = (lambda_coef*(norm(w)**2)) + lambda_factor*(norm(v)**2) / 2\n",
    "    total_loss = loss + regularization_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Function\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\hat{y}(\\textbf{x})\n",
    "&= w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} + \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we have prediction function\n",
    "$$\n",
    "\\begin{align}\n",
    "\\hat{y}(\\textbf{x}) = w_{0} + \\sum_{i=1}^{n} w_{i} x_{i} +  \\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however we see that in\n",
    "$$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n w_{ij} x_{i} x_{j}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "we have quite expensive computation\n",
    "```{python}\n",
    "# loop 1\n",
    "for i in blabla :\n",
    "  for j in i ..\n",
    "    do something\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Could we afford using only one loop instead of looping twice?\n",
    "Yes, we could.. however we should modify it.\n",
    "We could see that index  $j= i+1$, we could remove the term i by substracting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j} - \\frac{1}{2} \\sum_{i=1}^n \\langle \\textbf{v}_i , \\textbf{v}_{i} \\rangle x_{i} x_{i}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As of now, we could recall that in our interaction feature represented by some n-length latent factor\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle = \\sum_{f=1}^k v_{i,f} v_{j,f}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "plug back to our function :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j} \\right) - \\frac{1}{2}\\left( \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2}\\left(\\sum_{i=1}^n \\sum_{j=1}^n \\sum_{f=1}^k v_{i,f} v_{j,f} x_{i} x_{j}  -  \\sum_{i=1}^n \\sum_{f=1}^k v_{i,f} v_{i,f} x_{i} x_{i} \\right) \\\\\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{j=1}^n v_{j,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wee see that our index $j=i=1$, hence we can replace the j index with i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left(\\sum_{i=1}^n v_{i,f}x_{i} \\right) \\left( \\sum_{i=1}^n v_{i,f}x_{j} \\right) - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final Form : of Our Prediction Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^n \\sum_{j=i+1}^n \\langle \\textbf{v}_i, \\textbf{v}_{j} \\rangle x_{i} x_{j}\n",
    "&= \\frac{1}{2} \\sum_{f=1}^{k} \\left( \\left( \\sum_{i}^{n} v_{i,f}x_{i} \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 x_{i}^2 \\right)\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data,indptr, indices,w_0, w, v, n_factors,n_features,i):\n",
    "    \"\"\"\" Predicting data on single training samples, from sparse matrices \"\"\"\n",
    "\n",
    "    # use this for updating parameter\n",
    "    summed = np.zeros(n_factors)\n",
    "    summed_squared = np.zeros(n_factors)\n",
    "\n",
    "    # linear output w * x\n",
    "    y_pred = w_0\n",
    "\n",
    "    for index in range(indptr[i], indptr[i + 1]):\n",
    "        feature = indices[index]\n",
    "        y_pred += w[feature] * data[index]\n",
    "\n",
    "    # factor output\n",
    "    for factor in range(n_factors):\n",
    "        for index in range(indptr[i], indptr[i + 1]):\n",
    "            feature = indices[index]\n",
    "            term = v[factor, feature] * data[index]\n",
    "            summed[factor] += term\n",
    "            summed_squared[factor] += term * term\n",
    "\n",
    "        y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
    "\n",
    "\n",
    "    return y_pred, summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction on sample 1 :  [-13.42671964]\n"
     ]
    }
   ],
   "source": [
    "# predict on training samples 1\n",
    "pred,summed = predict(data= X_train_sparse.data,\n",
    "                      indptr = X_train_sparse.indptr,\n",
    "                      indices = X_train_sparse.indices,\n",
    "                      w_0 = intercept, w = w, v = v , n_factors = n_factor,\n",
    "                      n_features = X_train.shape[1],\n",
    "                      i = 1\n",
    "                      )\n",
    "print('Prediction on sample 1 : ', pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find optimal parameter above we will find using one of the famous optimization algorithm, **Gradient Descent**, and to perform Gradient Descent we have to find optimal parameter when derivative of cost function with respect to each parameter is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our objective is in composite function, $h(x) = f(g(x))$, to find the partial derivative with respect to each parameter we can use **chain rule**; $h'(x) = f'(g(x)).g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. **$w_0$**\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Derivation Detail of $w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align*}\n",
    "\\text{Objective}\n",
    "&= \\underset{w_0,w^*,v*}{min} \\left [\\cfrac{1}{2}\\sum_{n \\in D} \\left( y^{n}- \\left (w_0 + \\sum_{i=1} w_i.x_i^n +  \\frac{1}{2} \\sum_{f=1}^{k} ( \\left( \\sum_{i}^{n} v_{i,f}x_i^n \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 {x_i^n}^2 \\right)  \\right )^2 \\right ] + \\cfrac{\\lambda}{2} \\left [\\sum_{i}(w_i^2 +||v_i||^2 ) \\right ]\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1st Block**\n",
    "\n",
    "Now we take care of each derivative $\\left [ \\text{some function} \\right ]$ block\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{{\\partial}(\\cfrac{1}{2}\\sum_{n \\in D}  (y^{(n)}- \\hat{y}^{(n)} )^2)}{\\partial{w_0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Where we could identify :     \n",
    "- $\\hat{y}^{(n)}$ is our prediction function , we could call it as `g(x)`\n",
    "- $\\left [\n",
    "(\\cfrac{1}{2}\\sum_{n \\in D}  (y^{(n)}- \\hat{y}^{(n)} )^2)\\right ]$ is our `f(g(x))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, our derivation become\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{{\\partial}(\\cfrac{1}{2}\\sum_{n \\in D}  (y^{(n)}- \\hat{y}^{(n)} )^2)}{\\partial{w_0}}\n",
    "&=\n",
    "\\cfrac{1}{2} \\frac{\\partial{f(g(x))}}{\\partial{g(x)}}. \\frac{\\partial{g(x)}}{\\partial{w_0}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember derivative concept, if we have $f(x) = ax^n$, the derivative or $f'(x) = n(ax^{n-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    " \\begin{align*}\n",
    "\\frac{\\partial{f(g(x))}}{\\partial{g(x)}}\n",
    "&= 2 \\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)} )\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "-\\frac{\\partial{g(x)}}{\\partial{w_0}}\n",
    "&= \\frac{\\partial{\\left (w_0 + \\sum_{i=1} w_i.x_i^n +  \\frac{1}{2} \\sum_{f=1}^{k} ( \\left( \\sum_{i}^{n} v_{i,f}x_i^n \\right)^2  - \\sum_{i=1}^{n} v_{i,f}^2 {x_i^n}^2 \\right)}}{\\partial{w_0}} \\\\\n",
    "&= -1\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug back each component\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{{\\partial}(\\cfrac{1}{2}\\sum_{n \\in D}  (y^{(n)}- \\hat{y}^{(n)} )^2)}{\\partial{w_0}}\n",
    "&=\n",
    "\\cfrac{1}{2} \\frac{\\partial{f(g(x))}}{\\partial{g(x)}}. \\frac{\\partial{g(x)}}{\\partial{w_0}} \\\\\n",
    "&= \\cfrac{1}{2} 2 \\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)} ) . -1 \\\\\n",
    "&= -\\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)} )\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2nd Block**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{1}{2}\\frac{\\partial{\\cfrac{\\lambda}{2} \\left [\\sum_{i}(w_i^2 +||v_i||^2 ) \\right ]} }{\\partial{w_0}}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since we don't have $w_0$ term in our regularization, our derivation should be $0$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Arrange All Component**\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{\\partial \\text{Objective}}{\\partial{w_0}}\n",
    "&=\n",
    "\\frac{\\partial}{\\partial{w_0}}\n",
    "\\underset{}{}\n",
    "\\left [\n",
    "(\\cfrac{1}{2}\\sum_{n \\in D}  (y^{(n)}- \\hat{y}^{(n)})^2)\\right ] + \\frac{\\partial}{\\partial{w_0}} \\left [ \\cfrac{\\lambda}{2} \\sum_{i} (w_i^2 +||v_i||^2 ) \\right ] \\\\\n",
    "&= \\left [(2.\\cfrac{1}{2}\\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)}).-1 \\right ] + 0 \\\\\n",
    "&= -\\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)})  \\\\\n",
    "\\text{Multiply with -1} \\\\\n",
    "\\cfrac{\\partial \\text{Objective}}{\\partial{w_0}}\n",
    "&= \\sum_{n \\in D} (y^{(n)}- \\hat{y}^{(n)})\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\sum_{n \\in D}(y^{(n)} - \\hat{y}^{(n)})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. **$w_i$**\n",
    "\n",
    "\n",
    "Using the same procedure as above\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{\\partial \\text{Objective}}{\\partial{w_i}}\n",
    "&= \\sum_{n \\in D}(y^{(n)} - \\hat{y}^{(n)}).\\sum_i x_i^{(n)} +   (\\lambda \\sum_i w_i))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Remember :      \n",
    "- $ \\sum_i x_i^{(n)} = X^{n} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. **$v_{i,f}$**\n",
    "Suppose we use the same technique, we will endup with\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cfrac{\\partial \\text{Objective}}{\\partial{w_{i,f}}}\n",
    "&= \\sum_{n \\in D}(y^{(n)} - \\hat{y}^{(n)}). x_i \\sum_{f=1}^k \\sum_{j=1}^n v_{j,f}.x_j - v_{i,f}x_i^2 + \\lambda\\sum_i \\sum_f^k v_{i,f}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optimization Detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update the parameter to optimal we need other components :     \n",
    "\n",
    "- $\\gamma$ : Learning rate\n",
    "- n_epochs : number of iteration to update the parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How gradient descent work to update parameter :    \n",
    "For n_iterations do :    \n",
    "Loop all over training samples :     \n",
    "1. Find  partial derivative /gradient of objective function with respect to parameter we want to optimize ( in this case $b_u,b_i,p_u,q_i$)\n",
    "\n",
    "2. Update parameter with those gradient :     \n",
    "\n",
    "  $\\text{parameter}_{\\text{new}} = \\text{parameter}_{\\text{old}} -\\gamma.(\\text{gradient})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lot of alternatives to original **Gradient Descent**, such as :     \n",
    "1. Mini batch Gradient Descent\n",
    "2. Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "# regularization of coefficient\n",
    "lambda_coef = 0.002\n",
    "# regularization of interaction term\n",
    "lambda_factor = 0.002\n",
    "# learning rate\n",
    "learning_rate = 0.001\n",
    "# n_epoch\n",
    "n_epoch = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\894921705.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  v[factor, feature] += learning_rate * (v_gradient +  lambda_factor * v[factor, feature])\n",
      "100%|██████████| 100/100 [00:00<00:00, 238.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# implement stochastic gradient descent\n",
    "history_loss = []\n",
    "for epoch in tqdm(range(n_epoch)) :\n",
    "  # pick random sample (stochastic)\n",
    "  idx = np.arange(X_train.shape[0])\n",
    "  i = np.random.choice(idx)\n",
    "\n",
    "  # set loss for each epocj\n",
    "  loss = 0.0\n",
    "  # run prediction\n",
    "  y_pred, summed = predict(data= X_train_sparse.data, indptr= X_train_sparse.indptr,\n",
    "                                      indices= X_train_sparse.indices, w_0 = intercept, w= w,\n",
    "                                      v= v , n_factors = n_factor,\n",
    "                                      n_features = n_features,i=i)\n",
    "\n",
    "  # calculate loss and its gradient\n",
    "  loss += mse_loss (y_true= y_train[i], y_pred= y_pred, lambda_coef =lambda_coef,\n",
    "                    lambda_factor=lambda_factor,w=w, v= v)\n",
    "  # add loss\n",
    "  history_loss.append(loss)\n",
    "\n",
    "  # error gradient 1/2 * dy/dx (MSE)\n",
    "  error_gradient = (y_train[i] - y_pred)\n",
    "\n",
    "  # update_intercept\n",
    "  intercept += (learning_rate * error_gradient)\n",
    "\n",
    "  # update weight\n",
    "  # iterate through all pointer (row)\n",
    "  for index in range(X_train_sparse.indptr[i], X_train_sparse.indptr[i + 1]):\n",
    "      # slice indices (row) based on\n",
    "      feature = X_train_sparse.indices[index]\n",
    "      w[feature] += learning_rate * (error_gradient * X_train_sparse.data[index] + lambda_coef * w[feature])\n",
    "\n",
    "\n",
    "  # update each interaction factor\n",
    "  for factor in range(n_factor):\n",
    "      for index in range(X_train_sparse.indptr[i], X_train_sparse.indptr[i + 1]):\n",
    "          feature = X_train_sparse.indices[index]\n",
    "          term = summed[factor] - v[factor, feature] * X_train_sparse.data[index]\n",
    "          v_gradient = error_gradient * X_train_sparse.data[index] * term\n",
    "          v[factor, feature] += learning_rate * (v_gradient +  lambda_factor * v[factor, feature])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Object Oriented Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "\n",
    "class FMRegressor(BaseEstimator, RegressorMixin):\n",
    "    \"\"\"\n",
    "    Factorizaton Machines for Regression Task\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_epoch : int, default 100\n",
    "        Number of iterations\n",
    "\n",
    "    n_factors : int, default 20\n",
    "        Number of latent factor for feature interaction\n",
    "\n",
    "    lr : float, default 0.001\n",
    "        Learning rate for the Stochastic Gradient Descent\n",
    "\n",
    "    lambda_coef : float, default 0.1\n",
    "        Regularization strength for weights.\n",
    "\n",
    "    lambda_factor : float, default 0.1\n",
    "        Regularization strength for interaction latent factors.\n",
    "\n",
    "    random_state : int, default 49\n",
    "        Seed for reproducibility\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_epoch = 10, n_factors = 20,\n",
    "                 lr = 0.01, lambda_coef = 0.1,lambda_factor = 0.1,\n",
    "                 random_state = 49):\n",
    "        self.n_epoch = n_epoch\n",
    "        self.lambda_coef = lambda_coef\n",
    "        self.lambda_factor = lambda_factor\n",
    "        self.n_factors = n_factors\n",
    "        self.random_state = random_state\n",
    "        self.lr = lr\n",
    "\n",
    "    def initialize_parameters(self,n_features) :\n",
    "        \"\"\"Function to initialize parameters\"\"\"\n",
    "\n",
    "        # intercept --> w_0\n",
    "        self.intercept_ = 0.0\n",
    "\n",
    "        # weight --> w_i\n",
    "        self.coef_ = np.zeros(n_features)\n",
    "\n",
    "        # initialized interaction factor for each features (users,item,other features)\n",
    "        self.feature_factors_ = np.random.normal(0.,1.,size = (self.n_factors, n_features))\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the input data and label.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy sparse csr_matrix, shape <n_samples, n_features>\n",
    "            Data in sparse matrix format.\n",
    "\n",
    "        y : 1d ndarray, shape <n_samples,>\n",
    "            Training data's corresponding target (rating).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_samples, self.n_features = X.shape\n",
    "\n",
    "        # setting seed\n",
    "        np.random.seed(self.random_state)\n",
    "\n",
    "        self.initialize_parameters(n_features= self.n_features)\n",
    "\n",
    "\n",
    "        self.history_ = []\n",
    "        for epoch in tqdm(range(n_epoch)) :\n",
    "            loss = update_parameter(data = X.data,indptr= X.indptr, indices = X.indices, y = y ,\n",
    "                              n_samples=self.n_samples,\n",
    "                              n_features = self.n_features, w_0 = self.intercept_,\n",
    "                              w = self.coef_, v= self.feature_factors_,\n",
    "                              n_factors = self.n_factors, learning_rate = self.lr,\n",
    "                              lambda_coef= self.lambda_coef,lambda_factor = self.lambda_factor)\n",
    "            self.history_.append(loss)\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Prediction function for inference (after training)\"\"\"\n",
    "\n",
    "        # check instance\n",
    "        if isinstance(X,sp._csr.csr_matrix) :\n",
    "            # convert if X is sparse matrix to numpy matrix\n",
    "            X = X.toarray()\n",
    "\n",
    "        # W_i.x_i\n",
    "        linear_output = X.dot(self.coef_).reshape(1,-1)[0]\n",
    "\n",
    "        # feature interaction\n",
    "        v = self.feature_factors_.T\n",
    "        term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
    "        factor_output = 0.5 * np.sum(term, axis = 1)\n",
    "\n",
    "        # sum all prediction\n",
    "        y_pred = self.intercept_ + linear_output + factor_output\n",
    "\n",
    "        # implement clipping\n",
    "        for i in range(len(y_pred)) :\n",
    "          if y_pred[i] > 5 :\n",
    "            y_pred[i] = 5\n",
    "          elif y_pred[i]  < 1 :\n",
    "            y_pred[i]  = 1\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def update_parameter(data,indptr,indices,y,n_samples, n_features,\n",
    "                w_0, w, v, n_factors, learning_rate, lambda_coef ,lambda_factor):\n",
    "    \"\"\"\n",
    "    Update parameter\n",
    "    \"\"\"\n",
    "    # set loss\n",
    "    loss = 0.0\n",
    "\n",
    "    # pick random sample to update\n",
    "    idx = np.arange(n_samples)\n",
    "    i = np.random.choice(idx)\n",
    "\n",
    "    # generate prediction\n",
    "    y_pred, summed = _predict_instance(data= data, indptr= indptr,\n",
    "                                        indices= indices, w_0 = w_0, w= w,\n",
    "                                        v= v , n_factors = n_factors,\n",
    "                                        n_features = n_features,i=i)\n",
    "\n",
    "    # calculate loss and its gradient\n",
    "    loss += mse_loss (y_true= y[i], y_pred= y_pred, lambda_coef =lambda_coef,\n",
    "                      lambda_factor=lambda_factor,w=w, v= v)\n",
    "\n",
    "    # error gradient 1/2 * dy/dx (MSE)\n",
    "    error_gradient = (y[i] - y_pred)\n",
    "\n",
    "    # update_intercept\n",
    "    w_0 += (learning_rate * error_gradient)\n",
    "\n",
    "    # update weight\n",
    "    # iterate through all pointer (row)\n",
    "    for index in range(indptr[i], indptr[i + 1]):\n",
    "        # find feature that are not have zeros value (sparse)\n",
    "        feature = indices[index]\n",
    "        # update w\n",
    "        w[feature] += learning_rate * (error_gradient * data[index] + lambda_coef * w[feature])\n",
    "\n",
    "\n",
    "    # update each interaction factor\n",
    "    for factor in range(n_factors):\n",
    "        # iterate through all pointer (row)\n",
    "        for index in range(indptr[i], indptr[i + 1]):\n",
    "            # find feature that are not have zeros value (sparse)\n",
    "            feature = indices[index]\n",
    "            # calculate term\n",
    "            term = summed[factor] - v[factor, feature] * (data[index]**2)\n",
    "\n",
    "            # dy / dx\n",
    "            v_gradient = error_gradient * data[index] * term\n",
    "\n",
    "            v[factor, feature] += learning_rate * (v_gradient +  lambda_factor * v[factor, feature])\n",
    "\n",
    "    loss /= n_samples\n",
    "    return loss\n",
    "\n",
    "\n",
    "def _predict_instance(data,indptr, indices,w_0, w, v, n_factors,n_features,i):\n",
    "    \"\"\"predicting a single instance\"\"\"\n",
    "    summed = np.zeros(n_factors)\n",
    "    summed_squared = np.zeros(n_factors)\n",
    "\n",
    "    # linear output w * x\n",
    "    y_pred = w_0\n",
    "\n",
    "    for index in range(indptr[i], indptr[i + 1]):\n",
    "        feature = indices[index]\n",
    "        y_pred += w[feature] * data[index]\n",
    "\n",
    "    # factor output\n",
    "    for factor in range(n_factors):\n",
    "        for index in range(indptr[i], indptr[i + 1]):\n",
    "            feature = indices[index]\n",
    "            term = v[factor, feature] * data[index]\n",
    "            summed[factor] += term\n",
    "            summed_squared[factor] += term * term\n",
    "\n",
    "        y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
    "\n",
    "\n",
    "    return y_pred, summed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FMRegressor(n_epoch=1000,lambda_coef=0.01,lr=0.01,n_factors=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 479.19it/s]\n"
     ]
    }
   ],
   "source": [
    "%timeit\n",
    "model.fit(X_train_sparse,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.103573903355957,\n",
       " 2.136507927599461,\n",
       " 2.103049059879899,\n",
       " 2.1193881797312457,\n",
       " 2.1083262422989826,\n",
       " 2.1204941301550786,\n",
       " 2.105060869124429,\n",
       " 2.1030408805127476,\n",
       " 2.1067035819871007,\n",
       " 2.106168640353143,\n",
       " 2.134485999350442,\n",
       " 2.1450680132893494,\n",
       " 2.1047231656318246,\n",
       " 2.119392957670942,\n",
       " 2.161217107732948,\n",
       " 2.1038682542460565,\n",
       " 2.125228268678829,\n",
       " 2.1093575433773943,\n",
       " 2.138174494062725,\n",
       " 2.105489081134886,\n",
       " 2.107482642871969,\n",
       " 2.115176043509399,\n",
       " 2.1049654409038743,\n",
       " 2.1076751768648765,\n",
       " 2.104143050039917,\n",
       " 2.1060875857068018,\n",
       " 2.1036943630232843,\n",
       " 2.104299353428089,\n",
       " 2.1289877950385474,\n",
       " 2.106379419924681,\n",
       " 2.1230874134293334,\n",
       " 2.109664787352479,\n",
       " 2.1094709352695316,\n",
       " 2.1044701255657094,\n",
       " 2.1438244960722788,\n",
       " 2.1040429636979003,\n",
       " 2.1140084603116436,\n",
       " 2.11597913076335,\n",
       " 2.1570428692230843,\n",
       " 2.116903652848005,\n",
       " 2.1131704078581697,\n",
       " 2.1187938036781553,\n",
       " 2.1044833287078437,\n",
       " 2.1123018015345827,\n",
       " 2.10718618232965,\n",
       " 2.135716987082862,\n",
       " 2.1068041629820597,\n",
       " 2.177812545572116,\n",
       " 2.1091789793669142,\n",
       " 2.136301241273963,\n",
       " 2.1132259414750174,\n",
       " 2.125545533870439,\n",
       " 2.1050997907432105,\n",
       " 2.105558432328123,\n",
       " 2.105467283542914,\n",
       " 2.109599130534845,\n",
       " 2.1526091513146284,\n",
       " 2.1138733433283283,\n",
       " 2.1056387953646625,\n",
       " 2.1119041461994366,\n",
       " 2.106478906901197,\n",
       " 2.108748387810018,\n",
       " 2.105695305639071,\n",
       " 2.114070921636687,\n",
       " 2.1471926328627147,\n",
       " 2.1294532978715726,\n",
       " 2.1081973882563556,\n",
       " 2.210410038825026,\n",
       " 2.15312078173207,\n",
       " 2.1706341781394802,\n",
       " 2.1085877081553313,\n",
       " 2.107283989219305,\n",
       " 2.1075525178392276,\n",
       " 2.1135085222188468,\n",
       " 2.134678732948777,\n",
       " 2.1771658246505563,\n",
       " 2.1063277748738067,\n",
       " 2.1069586650160352,\n",
       " 2.1255512881769425,\n",
       " 2.1141511529695602,\n",
       " 2.1176681910299355,\n",
       " 2.1759726690591803,\n",
       " 2.1413433071048247,\n",
       " 2.114495727251116,\n",
       " 2.1102152723082277,\n",
       " 2.1077552002363573,\n",
       " 2.1474305190221363,\n",
       " 2.1180347027251907,\n",
       " 2.112689038544376,\n",
       " 2.119041794008655,\n",
       " 2.109487664349676,\n",
       " 2.109749214841626,\n",
       " 2.7485098323575445,\n",
       " 2.131802324297536,\n",
       " 2.114710874397449,\n",
       " 2.1118852515220587,\n",
       " 2.276470381004306,\n",
       " 2.151907017272382,\n",
       " 2.144569388339192,\n",
       " 2.1184911113023035]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.history_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1b8eaf35360>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot([x for x in range(len(model.history_))],model.history_,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 1., 1., ..., 5., 5., 1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_train_sparse)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5017739556046976"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.sqrt(mean_squared_error(y_train,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already train our `Factorization Machine Models`, now we want to generate prediction on  one of our user. We want to predict the rating that will be given by user `5C28F393B23BB894523AE7126A7AE445` on item `219668`, on context `SOLO` trip.\n",
    "Previously, we map the the user and item id, so we retrieve it first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_userid = 'CAFAF8275C721F57AB58686ECC5AEC41'\n",
    "target_itemid = 1149361\n",
    "target_context = 'SOLO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped User id : 16, Item id : 10\n"
     ]
    }
   ],
   "source": [
    "# retrieve mapping\n",
    "user_id = user_to_id[target_userid]\n",
    "item_id = item_to_id[target_itemid]\n",
    "\n",
    "print(f'Mapped User id : {user_id}, Item id : {item_id}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, see that our input for each training data consist of `<4645>` features, we need to process to create format data for inference / prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use the same format as our `processed_data`, and assign values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_0</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_2264</th>\n",
       "      <th>item_id_2265</th>\n",
       "      <th>item_id_2266</th>\n",
       "      <th>item_id_2267</th>\n",
       "      <th>item_id_2268</th>\n",
       "      <th>trip_BUSINESS</th>\n",
       "      <th>trip_COUPLES</th>\n",
       "      <th>trip_FAMILY</th>\n",
       "      <th>trip_FRIENDS</th>\n",
       "      <th>trip_SOLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_0  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5   \n",
       "1          0          1          0          0          0          0  \\\n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  item_id_2264   \n",
       "1          0          0          0          0  ...             0  \\\n",
       "\n",
       "   item_id_2265  item_id_2266  item_id_2267  item_id_2268  trip_BUSINESS   \n",
       "1             0             0             0             0              0  \\\n",
       "\n",
       "   trip_COUPLES  trip_FAMILY  trip_FRIENDS  trip_SOLO  \n",
       "1             0            0             0          1  \n",
       "\n",
       "[1 rows x 4645 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prediction_format = X.iloc[[1]]\n",
    "data_prediction_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id_0</th>\n",
       "      <th>user_id_1</th>\n",
       "      <th>user_id_2</th>\n",
       "      <th>user_id_3</th>\n",
       "      <th>user_id_4</th>\n",
       "      <th>user_id_5</th>\n",
       "      <th>user_id_6</th>\n",
       "      <th>user_id_7</th>\n",
       "      <th>user_id_8</th>\n",
       "      <th>user_id_9</th>\n",
       "      <th>...</th>\n",
       "      <th>item_id_2264</th>\n",
       "      <th>item_id_2265</th>\n",
       "      <th>item_id_2266</th>\n",
       "      <th>item_id_2267</th>\n",
       "      <th>item_id_2268</th>\n",
       "      <th>trip_BUSINESS</th>\n",
       "      <th>trip_COUPLES</th>\n",
       "      <th>trip_FAMILY</th>\n",
       "      <th>trip_FRIENDS</th>\n",
       "      <th>trip_SOLO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 4645 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id_0  user_id_1  user_id_2  user_id_3  user_id_4  user_id_5   \n",
       "1          0          0          0          0          0          0  \\\n",
       "\n",
       "   user_id_6  user_id_7  user_id_8  user_id_9  ...  item_id_2264   \n",
       "1          0          0          0          0  ...             0  \\\n",
       "\n",
       "   item_id_2265  item_id_2266  item_id_2267  item_id_2268  trip_BUSINESS   \n",
       "1             0             0             0             0              0  \\\n",
       "\n",
       "   trip_COUPLES  trip_FAMILY  trip_FRIENDS  trip_SOLO  \n",
       "1             0            0             0          0  \n",
       "\n",
       "[1 rows x 4645 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace all data with 0\n",
    "data_prediction_format.loc[:] = 0\n",
    "data_prediction_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now will fill the user_id from our target with 1\n",
    "data_prediction_format.loc[:,f'user_id_{user_id}'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now will fill the item_id from our target with 1\n",
    "data_prediction_format.loc[:,f'item_id_{item_id}'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now will fill the trip_context from our target with 1\n",
    "data_prediction_format.loc[:,f'trip_{target_context}'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will convert into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_samples = data_prediction_format.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run prediction\n",
    "model.predict(X_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, later we want to generate prediction, we will create function to prepare inference / prediction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prediction_input(user_id, item_id, context,\n",
    "                             data_prediction_format = data_prediction_format) :\n",
    "    \"\"\"Function to prepare prediction / inference data\"\"\"\n",
    "    # copy\n",
    "    data_prediction_format = data_prediction_format.copy()\n",
    "\n",
    "    # assign all values from each column to zero\n",
    "    data_prediction_format.loc[:,] = 0\n",
    "\n",
    "    # now will fill the user_id\n",
    "    data_prediction_format.loc[:,f'user_id_{user_id}'] = 1\n",
    "\n",
    "    # now will fill the item_id\n",
    "    data_prediction_format.loc[:,f'item_id_{item_id}'] = 1\n",
    "\n",
    "    # now will fill the trip_context\n",
    "    data_prediction_format.loc[:,f'trip_{target_context}'] = 1\n",
    "\n",
    "    # return numpy data\n",
    "    return data_prediction_format.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.576658922044949"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.sqrt(mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Recommender System Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Model on Factorization Machine we are going to use polylearn package, which is under sklearn package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we already satistified with `Factorization Machines` requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train all model candidate with its hyperparameter so that we can compare which model + settings yield good result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1UzWBpL1ycd2eyKYC3gJzmdBKyE1Zbefy\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since *Hyperparameter* Is not yielded through learning process, we have to find it / set it to yield optimal model performance. Some methods  Hyperparameter Tuning :    \n",
    "\n",
    "- GridSearchCV\n",
    "\n",
    "  Fitting model through all combinations of hyperparameter values and compare each fit → which combinations yield the best objective\n",
    "\n",
    "- RandomizedSearchCV\n",
    "\n",
    "  Fitting Model only through sampled hyperparameter candidates. Much more efficient than GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning require **Cross Validation** , making sure the during hyperparameter selection the performance measured is unbias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Cross Validation</center>\n",
    "<center><img src=\"https://drive.google.com/uc?id=1Sa1vKjku_pI6WZoYfKqX7lmNeYcpxZNz\" width=600></center>\n",
    "<center><a href=\"https://scikit-learn.org/stable/modules/cross_validation.html\">Source</a></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter** in best models\n",
    "to identify the hyperparameters in models, we need to read its documentation/ paper first,for **surprise** model the model documentation is [here](https://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline)\n",
    "Hyperparameters :    \n",
    "\n",
    "- *k* (Number of Neigbors)\n",
    "- similarity function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cross Validation** Method\n",
    "\n",
    "for this we will choose the **K-Fold** Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Method** : *RandomizedSearchCV*\n",
    "\n",
    "due to high computational cost if we use *GridSearchCV* we we will use *RandomizedSearchCV* ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run hyperparameter tuning, **surprise** package already have builtin function to run hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Candidate**\n",
    "\n",
    "1. Baseline Model ( Mean Prediction)  (does not have hyperparameter)\n",
    "2. Factorization Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Model Object**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor\n",
    "\n",
    "model_baseline = DummyRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fit Baseline Model**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyRegressor</label><div class=\"sk-toggleable__content\"><pre>DummyRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyRegressor()"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_baseline.fit(X_train_sparse,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for baseline_model :  0.933334502313835\n"
     ]
    }
   ],
   "source": [
    "# Calculate Score (MSE) from baseline model\n",
    "baseline_pred = model_baseline.predict(X_train)\n",
    "\n",
    "baseline_mse = np.sqrt(mean_squared_error(y_train,baseline_pred))\n",
    "\n",
    "# print result\n",
    "print('RMSE for baseline_model : ',baseline_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for baseline_model :  0.9447043396371422\n"
     ]
    }
   ],
   "source": [
    "# predict on test data\n",
    "baseline_pred = model_baseline.predict(X_test)\n",
    "\n",
    "baseline_rmse_test = np.sqrt(mean_squared_error(y_test,baseline_pred))\n",
    "\n",
    "# print result\n",
    "print('Test RMSE for baseline_model : ',baseline_rmse_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Funk SVD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Original data shape : (14175, 4)\n",
      "- After dropping duplicate data shape :  (13849, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0        0       5\n",
       "1        1        1       5\n",
       "2        2        2       5\n",
       "3        1        3       5\n",
       "4        3        4       5"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load our data\n",
    "trip_data = load_trip_data(path = trip_data_path)\n",
    "trip_data,user_mapping,item_mapping = map_data(trip_data)\n",
    "\n",
    "# drop trip column\n",
    "trip_data = trip_data.drop('trip',axis=1)\n",
    "trip_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: surprise in c:\\users\\daniel.satria\\anaconda3\\envs\\nlp\\lib\\site-packages (0.1)\n",
      "Requirement already satisfied: scikit-surprise in c:\\users\\daniel.satria\\anaconda3\\envs\\nlp\\lib\\site-packages (from surprise) (1.1.3)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\daniel.satria\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-surprise->surprise) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\daniel.satria\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-surprise->surprise) (1.26.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\daniel.satria\\anaconda3\\envs\\nlp\\lib\\site-packages (from scikit-surprise->surprise) (1.10.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.reader.Reader at 0x1b8eafbb5b0>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import some library\n",
    "from surprise import Dataset, Reader\n",
    "\n",
    "# Initiating rating scale\n",
    "reader = Reader(rating_scale = (1, 5))\n",
    "reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate the data. It must be on format `userId`, `itemId`, and `ratings`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.dataset.DatasetAutoFolds at 0x1b8ed629ba0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utility_data = Dataset.load_from_df(\n",
    "                    df = trip_data[['user_id', 'item_id', 'rating']].copy(),\n",
    "                    reader = reader\n",
    "                )\n",
    "\n",
    "utility_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library for deep copy\n",
    "import copy\n",
    "\n",
    "# Create a function\n",
    "def train_test_split(utility_data, test_size, random_state):\n",
    "    \"\"\"\n",
    "    Train test split the data\n",
    "    ref: https://surprise.readthedocs.io/en/stable/FAQ.html#split-data-for-unbiased-estimation-py\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    utility_data : Surprise utility data\n",
    "        The sample of whole data set\n",
    "\n",
    "    test_size : float, default=0.2\n",
    "        The test size\n",
    "\n",
    "    random_state : int, default=42\n",
    "        For reproducibility\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    full_data : Surprise utility data\n",
    "        The new utility data\n",
    "\n",
    "    train_data : Surprise format\n",
    "        The train data\n",
    "\n",
    "    test_data : Surprise format\n",
    "        The test data\n",
    "    \"\"\"\n",
    "    # Deep copy the utility_data\n",
    "    full_data = copy.deepcopy(utility_data)\n",
    "\n",
    "    # Generate random seed\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    # Shuffle the raw_ratings for reproducibility\n",
    "    raw_ratings = full_data.raw_ratings\n",
    "    np.random.shuffle(raw_ratings)\n",
    "\n",
    "    # Define the threshold\n",
    "    threshold = int((1-test_size) * len(raw_ratings))\n",
    "\n",
    "    # Split the data\n",
    "    train_raw_ratings = raw_ratings[:threshold]\n",
    "    test_raw_ratings = raw_ratings[threshold:]\n",
    "\n",
    "    # Get the data\n",
    "    full_data.raw_ratings = train_raw_ratings\n",
    "    train_data = full_data.build_full_trainset()\n",
    "    test_data = full_data.construct_testset(test_raw_ratings)\n",
    "\n",
    "    return full_data, train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "full_data, train_data, test_data = train_test_split(utility_data,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11079, 2770)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate the splitting\n",
    "train_data.n_ratings, len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing package\n",
    "from surprise.model_selection.search import RandomizedSearchCV\n",
    "\n",
    "# Load the model library\n",
    "# i.e. SVD\n",
    "from surprise import  SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter that are in Funk SVD models are :     \n",
    "- $\\gamma$ : Learning Rate\n",
    "- n_factors : Number of letent factor\n",
    "- $\\lambda$ : Regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Candidate :     \n",
    "1. Learning Rate ($\\gamma$)\n",
    "\n",
    "For this experiment we will try several values\n",
    "\n",
    "[1,0.1,0.01,0.001,0.0001]\n",
    "\n",
    "2. Number of latent factors\n",
    "For this experiment we will try some values :    \n",
    "\n",
    "[50,100,200]\n",
    "\n",
    "3. Regularization Strength\n",
    "\n",
    "[1,0.1,0.01,0.02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of parameter\n",
    "params_SVD = {'lr_all' : [1,0.1,0.01,0.001,0.0001], 'n_factors' : [50,100,200],\n",
    "              'reg_all' : [1,0.1,0.01,0.02]\n",
    "              }\n",
    "\n",
    "tuning_svd = RandomizedSearchCV(algo_class=SVD, param_distributions = params_SVD,\n",
    "                   cv=5\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_svd.fit(data=full_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Factorization Machine Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are gonna use, our previous model created from scratch, However there are still solution such as package :    \n",
    "\n",
    "1. [FastFM](https://github.com/ibayer/fastFM)\n",
    "2. [Polylearn](https://github.com/scikit-learn-contrib/polylearn)\n",
    "3. [Pytorch fm](https://github.com/rixwew/pytorch-fm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Factorization Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Factorization Machines Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter that are in Factorization Machines   are :     \n",
    "- $\\gamma$ : Learning Rate\n",
    "- n_factors : Number of letent factor\n",
    "- $\\lambda$ : Regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Candidate :     \n",
    "1. Learning Rate ($\\gamma$)\n",
    "\n",
    "For this experiment we will try several values\n",
    "\n",
    "[1,0.1,0.01,0.001,0.0001]\n",
    "\n",
    "2. Number of latent factors\n",
    "For this experiment we will try some values :    \n",
    "\n",
    "[50,100,200]\n",
    "\n",
    "3. Regularization Strength (lambda_coef & lambda_factor)\n",
    "\n",
    "[1,0.1,0.01,0.02]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciate model\n",
    "model_fm = FMRegressor()\n",
    "\n",
    "#create dictionary of parameter\n",
    "params_fm = {'lr' : [1,0.1,0.01,0.001,0.0001], 'n_factors' : [50,100,200],\n",
    "              'lambda_factor' : [1,0.1,0.01,0.02], 'lambda_coef' : [1,0.1,0.01,0.02]\n",
    "              }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameter Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing randomizedsearch to find best parameter\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# instanciate model selection\n",
    "cv_fm = RandomizedSearchCV(estimator=model_fm,\n",
    "                           param_distributions=params_fm,\n",
    "                           scoring = 'neg_root_mean_squared_error',\n",
    "                           cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 761.92it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 489.86it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 656.76it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 851.89it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 912.90it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 809.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 716.96it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 864.92it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 920.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 580.52it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 332.19it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 541.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 563.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 537.41it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 448.99it/s]\n",
      " 88%|████████▊ | 88/100 [00:00<00:00, 269.81it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 268.05it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 286.10it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 123.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 231.91it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 261.05it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 856.88it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 872.11it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 699.09it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 676.68it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 941.85it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 455.62it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      " 73%|███████▎  | 73/100 [00:00<00:00, 302.97it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 327.54it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      " 77%|███████▋  | 77/100 [00:00<00:00, 212.62it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 197.92it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      " 53%|█████▎    | 53/100 [00:00<00:00, 526.66it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\800810812.py:6: RuntimeWarning: overflow encountered in scalar power\n",
      "  loss = ((y_true - y_pred)**2) / 2\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 542.81it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 454.06it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 509.01it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 508.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 493.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 557.48it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 332.15it/s]\n",
      " 85%|████████▌ | 85/100 [00:00<00:00, 225.41it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 231.62it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 289.54it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 284.12it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 242.57it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 260.69it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 568.23it/s]\n",
      " 42%|████▏     | 42/100 [00:00<00:00, 414.04it/s]C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:192: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  summed_squared[factor] += term * term\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: overflow encountered in scalar multiply\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:194: RuntimeWarning: invalid value encountered in scalar subtract\n",
      "  y_pred += 0.5 * (summed[factor] * summed[factor] - summed_squared[factor])\n",
      "100%|██████████| 100/100 [00:00<00:00, 311.58it/s]\n",
      "C:\\Users\\daniel.satria\\AppData\\Local\\Temp\\ipykernel_2716\\244928946.py:102: RuntimeWarning: overflow encountered in square\n",
      "  term = (X.dot(v)) ** 2 - ((X**2).dot(v ** 2))\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:778: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 767, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 234, in __call__\n",
      "    return self._score(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 282, in _score\n",
      "    return self._sign * self._score_func(y_true, y_pred, **self._kwargs)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 442, in mean_squared_error\n",
      "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\metrics\\_regression.py\", line 102, in _check_reg_targets\n",
      "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 921, in check_array\n",
      "    _assert_all_finite(\n",
      "  File \"c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 161, in _assert_all_finite\n",
      "    raise ValueError(msg_err)\n",
      "ValueError: Input contains NaN.\n",
      "\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 539.79it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 400.47it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 532.03it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 318.33it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 291.54it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 299.13it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 312.70it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 312.72it/s]\n",
      "c:\\Users\\daniel.satria\\Anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [        nan -2.52215365 -2.57024218         nan -2.58220332         nan\n",
      " -2.56972763         nan         nan -2.53607027]\n",
      "  warnings.warn(\n",
      "100%|██████████| 100/100 [00:00<00:00, 798.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=FMRegressor(),\n",
       "                   param_distributions={&#x27;lambda_coef&#x27;: [1, 0.1, 0.01, 0.02],\n",
       "                                        &#x27;lambda_factor&#x27;: [1, 0.1, 0.01, 0.02],\n",
       "                                        &#x27;lr&#x27;: [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                                        &#x27;n_factors&#x27;: [50, 100, 200]},\n",
       "                   scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=FMRegressor(),\n",
       "                   param_distributions={&#x27;lambda_coef&#x27;: [1, 0.1, 0.01, 0.02],\n",
       "                                        &#x27;lambda_factor&#x27;: [1, 0.1, 0.01, 0.02],\n",
       "                                        &#x27;lr&#x27;: [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                                        &#x27;n_factors&#x27;: [50, 100, 200]},\n",
       "                   scoring=&#x27;neg_root_mean_squared_error&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: FMRegressor</label><div class=\"sk-toggleable__content\"><pre>FMRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FMRegressor</label><div class=\"sk-toggleable__content\"><pre>FMRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=FMRegressor(),\n",
       "                   param_distributions={'lambda_coef': [1, 0.1, 0.01, 0.02],\n",
       "                                        'lambda_factor': [1, 0.1, 0.01, 0.02],\n",
       "                                        'lr': [1, 0.1, 0.01, 0.001, 0.0001],\n",
       "                                        'n_factors': [50, 100, 200]},\n",
       "                   scoring='neg_root_mean_squared_error')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit hyperparameter tuning\n",
    "cv_fm.fit(X_train_sparse,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_factors': 50, 'lr': 0.01, 'lambda_factor': 0.01, 'lambda_coef': 0.02}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.5221536546802072"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fm.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we run experiment to our model, its better to recap the performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV Performance - RMSE</th>\n",
       "      <th>Model Condiguration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.966092</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Funk SVD</td>\n",
       "      <td>0.936941</td>\n",
       "      <td>{'lr_all': 0.0001, 'n_factors': 100, 'reg_all'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Factorization Machines</td>\n",
       "      <td>2.522154</td>\n",
       "      <td>{'n_factors': 50, 'lr': 0.01, 'lambda_factor':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model  CV Performance - RMSE   \n",
       "0                Baseline               0.966092  \\\n",
       "1                Funk SVD               0.936941   \n",
       "2  Factorization Machines               2.522154   \n",
       "\n",
       "                                 Model Condiguration  \n",
       "0                                                N/A  \n",
       "1  {'lr_all': 0.0001, 'n_factors': 100, 'reg_all'...  \n",
       "2  {'n_factors': 50, 'lr': 0.01, 'lambda_factor':...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_df = pd.DataFrame({'Model': ['Baseline', 'Funk SVD','Factorization Machines'],\n",
    "                           'CV Performance - RMSE': [np.sqrt(baseline_mse),tuning_svd.best_score['rmse'],-1*cv_fm.best_score_ ],\n",
    "                           'Model Condiguration':['N/A',f'{tuning_svd.best_params[\"rmse\"]}',\n",
    "                                                  f'{cv_fm.best_params_}']})\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that even though our Factorization Machines can handle `context` data however the performances is still under `Funk SVD` model, likely its because :    \n",
    "- Model Complexity (Interaction Term)\n",
    "- Less Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Best Model\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1b8ed24fd30>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create object\n",
    "best_params_svd = tuning_svd.best_params[\"rmse\"]\n",
    "model_best = SVD(**best_params_svd)\n",
    "\n",
    "# Retrain on whole train dataset\n",
    "model_best.fit(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9565282853545141"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import performance library\n",
    "from surprise import accuracy\n",
    "\n",
    "# predicting the test set using our best model\n",
    "test_pred = model_best.test(test_data)\n",
    "test_rmse = accuracy.rmse(test_pred)\n",
    "test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE-Tuning</th>\n",
       "      <th>RMSE-Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Funk SVD</td>\n",
       "      <td>0.936941</td>\n",
       "      <td>0.956528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  RMSE-Tuning  RMSE-Test\n",
       "0  Funk SVD     0.936941   0.956528"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_test_df = pd.DataFrame({'Model' : ['Funk SVD'],\n",
    "                                'RMSE-Tuning': [tuning_svd.best_score['rmse']],\n",
    "                                'RMSE-Test': [test_rmse]})\n",
    "\n",
    "summary_test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.Predictions / Generating Recommendation**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Process is to recommend items to user\n",
    "from our trained model\n",
    "\n",
    "<image src=\"https://www.mdpi.com/applsci/applsci-10-05510/article_deploy/html/images/applsci-10-05510-g001.png\" image>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "1. User to User Collaborative Filtering\n",
    "\n",
    "  We already trained our best model on full dataset, now its time to utilize our model to generate recommendation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we generate recommendation ?\n",
    "\n",
    "\n",
    "To generate recommendation :\n",
    "- predict all movies or only unseen movies rating from given users\n",
    "- followed by ordering the movies by its predicted rat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict with Best Model : User to User Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method predict in module surprise.prediction_algorithms.algo_base:\n",
      "\n",
      "predict(uid, iid, r_ui=None, clip=True, verbose=False) method of surprise.prediction_algorithms.matrix_factorization.SVD instance\n",
      "    Compute the rating prediction for given user and item.\n",
      "    \n",
      "    The ``predict`` method converts raw ids to inner ids and then calls the\n",
      "    ``estimate`` method which is defined in every derived class. If the\n",
      "    prediction is impossible (e.g. because the user and/or the item is\n",
      "    unknown), the prediction is set according to\n",
      "    :meth:`default_prediction()\n",
      "    <surprise.prediction_algorithms.algo_base.AlgoBase.default_prediction>`.\n",
      "    \n",
      "    Args:\n",
      "        uid: (Raw) id of the user. See :ref:`this note<raw_inner_note>`.\n",
      "        iid: (Raw) id of the item. See :ref:`this note<raw_inner_note>`.\n",
      "        r_ui(float): The true rating :math:`r_{ui}`. Optional, default is\n",
      "            ``None``.\n",
      "        clip(bool): Whether to clip the estimation into the rating scale.\n",
      "            For example, if :math:`\\hat{r}_{ui}` is :math:`5.5` while the\n",
      "            rating scale is :math:`[1, 5]`, then :math:`\\hat{r}_{ui}` is\n",
      "            set to :math:`5`. Same goes if :math:`\\hat{r}_{ui} < 1`.\n",
      "            Default is ``True``.\n",
      "        verbose(bool): Whether to print details of the prediction.  Default\n",
      "            is False.\n",
      "    \n",
      "    Returns:\n",
      "        A :obj:`Prediction            <surprise.prediction_algorithms.predictions.Prediction>` object\n",
      "        containing:\n",
      "    \n",
      "        - The (raw) user id ``uid``.\n",
      "        - The (raw) item id ``iid``.\n",
      "        - The true rating ``r_ui`` (:math:`r_{ui}`).\n",
      "        - The estimated rating (:math:`\\hat{r}_{ui}`).\n",
      "        - Some additional details about the prediction that might be useful\n",
      "          for later analysis.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Recommendation based on Best Models\n",
    "# We will try to recommend on sample userid ,userId 1 & 99\n",
    "\n",
    "# We can use model_best.predict method\n",
    "help(model_best.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_best.predict` has argument\n",
    "- `uid` (i.e., the `user_id`) and\n",
    "- `iid` (i.e., the item ID or `item_id`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting what is the rating of user 9 to movie 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prediction = model_best.predict(uid = 9,\n",
    "                                      iid = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(uid=9, iid=10, r_ui=None, est=4.187284931809498, details={'was_impossible': False})"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results tell us\n",
    "- `r_ui` : actual rating --> `None`, means user 9 have yet rated movie 10\n",
    "- `est` : the estimated rating from our model\n",
    "- `details` : whether prediction is impossible or not. So it's possible to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's predict all the unseen/unrated movie by userId 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14170</th>\n",
       "      <td>2159</td>\n",
       "      <td>1089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14171</th>\n",
       "      <td>2220</td>\n",
       "      <td>202</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14172</th>\n",
       "      <td>2224</td>\n",
       "      <td>791</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14173</th>\n",
       "      <td>2223</td>\n",
       "      <td>2268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14174</th>\n",
       "      <td>2232</td>\n",
       "      <td>1233</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13849 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  item_id  rating\n",
       "0            0        0       5\n",
       "1            1        1       5\n",
       "2            2        2       5\n",
       "3            1        3       5\n",
       "4            3        4       5\n",
       "...        ...      ...     ...\n",
       "14170     2159     1089       1\n",
       "14171     2220      202       1\n",
       "14172     2224      791       1\n",
       "14173     2223     2268       1\n",
       "14174     2232     1233       1\n",
       "\n",
       "[13849 rows x 3 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trip_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268}\n"
     ]
    }
   ],
   "source": [
    "# Get unique item_id\n",
    "unique_item_id = set(trip_data['item_id'])\n",
    "print(unique_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{32, 290, 1578, 17, 25}\n"
     ]
    }
   ],
   "source": [
    "# Get movieId that is rated by user id 9\n",
    "rated_item_id = set(trip_data.loc[trip_data['user_id']==9, 'item_id'])\n",
    "print(rated_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268}\n"
     ]
    }
   ],
   "source": [
    "#find unrated item_id\n",
    "# unrateddId = wholeId - ratedId\n",
    "unrated_item_id = unique_item_id.difference(rated_item_id)\n",
    "print(unrated_item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a function\n",
    "def get_unrated_item(userid, trip_data):\n",
    "    \"\"\"\n",
    "    Get unrated item id from a user id\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    userid : int\n",
    "        The user id\n",
    "\n",
    "    trip_data : pandas DataFrame\n",
    "        The trip_data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    unrated_item_id : set\n",
    "        The unrated item id\n",
    "    \"\"\"\n",
    "    # Find the whole item id\n",
    "    unique_item_id = set(trip_data['item_id'])\n",
    "\n",
    "    # Find the item id that was rated by user id\n",
    "    rated_item_id = set(trip_data.loc[trip_data['user_id']==userid, 'item_id'])\n",
    "\n",
    "    # Find the unrated item id\n",
    "    unrated_item_id = unique_item_id.difference(rated_item_id)\n",
    "\n",
    "    return unrated_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268}\n"
     ]
    }
   ],
   "source": [
    "unrated_item_id = get_unrated_item(userid=9, trip_data=trip_data)\n",
    "print(unrated_item_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Later, we create the prediction from the unrated movie**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 9, 'item_id': [], 'predicted_rating': []}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dict\n",
    "predicted_unrated_travel = {\n",
    "    'user_id': 9,\n",
    "    'item_id': [],\n",
    "    'predicted_rating': []\n",
    "}\n",
    "\n",
    "predicted_unrated_travel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for over all unrated item  Id\n",
    "for id in unrated_item_id:\n",
    "    # Create a prediction\n",
    "    pred_id = model_best.predict(uid = predicted_unrated_travel['user_id'],\n",
    "                                 iid = id)\n",
    "\n",
    "    # Append\n",
    "    predicted_unrated_travel['item_id'].append(id)\n",
    "    predicted_unrated_travel['predicted_rating'].append(pred_id.est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4.323860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4.133383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4.283946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4.178142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>4.207888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2259</th>\n",
       "      <td>9</td>\n",
       "      <td>2264</td>\n",
       "      <td>4.284310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2260</th>\n",
       "      <td>9</td>\n",
       "      <td>2265</td>\n",
       "      <td>4.161560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2261</th>\n",
       "      <td>9</td>\n",
       "      <td>2266</td>\n",
       "      <td>4.225257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2262</th>\n",
       "      <td>9</td>\n",
       "      <td>2267</td>\n",
       "      <td>4.057511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2263</th>\n",
       "      <td>9</td>\n",
       "      <td>2268</td>\n",
       "      <td>4.261424</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  predicted_rating\n",
       "0           9        0          4.323860\n",
       "1           9        1          4.133383\n",
       "2           9        2          4.283946\n",
       "3           9        3          4.178142\n",
       "4           9        4          4.207888\n",
       "...       ...      ...               ...\n",
       "2259        9     2264          4.284310\n",
       "2260        9     2265          4.161560\n",
       "2261        9     2266          4.225257\n",
       "2262        9     2267          4.057511\n",
       "2263        9     2268          4.261424\n",
       "\n",
       "[2264 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert to dataframe\n",
    "predicted_unrated_travel_df = pd.DataFrame(predicted_unrated_travel)\n",
    "predicted_unrated_travel_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2264.0</td>\n",
       "      <td>2264.000000</td>\n",
       "      <td>2264.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1135.646643</td>\n",
       "      <td>4.164486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>654.317550</td>\n",
       "      <td>0.110711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.701496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>569.750000</td>\n",
       "      <td>4.092829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1135.500000</td>\n",
       "      <td>4.161560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9.0</td>\n",
       "      <td>1702.250000</td>\n",
       "      <td>4.235595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2268.000000</td>\n",
       "      <td>4.536968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id      item_id  predicted_rating\n",
       "count   2264.0  2264.000000       2264.000000\n",
       "mean       9.0  1135.646643          4.164486\n",
       "std        0.0   654.317550          0.110711\n",
       "min        9.0     0.000000          3.701496\n",
       "25%        9.0   569.750000          4.092829\n",
       "50%        9.0  1135.500000          4.161560\n",
       "75%        9.0  1702.250000          4.235595\n",
       "max        9.0  2268.000000          4.536968"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_unrated_travel_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>9</td>\n",
       "      <td>2026</td>\n",
       "      <td>4.536968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>9</td>\n",
       "      <td>1498</td>\n",
       "      <td>4.530275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>9</td>\n",
       "      <td>924</td>\n",
       "      <td>4.530151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>9</td>\n",
       "      <td>2195</td>\n",
       "      <td>4.520749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>9</td>\n",
       "      <td>208</td>\n",
       "      <td>4.513122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>9</td>\n",
       "      <td>700</td>\n",
       "      <td>3.807598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>9</td>\n",
       "      <td>1178</td>\n",
       "      <td>3.801894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>9</td>\n",
       "      <td>2065</td>\n",
       "      <td>3.796135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>3.716102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>9</td>\n",
       "      <td>288</td>\n",
       "      <td>3.701496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  predicted_rating\n",
       "2021        9     2026          4.536968\n",
       "1494        9     1498          4.530275\n",
       "920         9      924          4.530151\n",
       "2190        9     2195          4.520749\n",
       "205         9      208          4.513122\n",
       "...       ...      ...               ...\n",
       "696         9      700          3.807598\n",
       "1174        9     1178          3.801894\n",
       "2060        9     2065          3.796135\n",
       "32          9       35          3.716102\n",
       "285         9      288          3.701496\n",
       "\n",
       "[2264 rows x 3 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort the predicted rating values\n",
    "predicted_unrated_movie = predicted_unrated_travel_df.sort_values('predicted_rating',\n",
    "                                                              ascending = False)\n",
    "\n",
    "predicted_unrated_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create this into a function\n",
    "def get_pred_unrated_item(userid, estimator, unrated_item_id):\n",
    "    \"\"\"\n",
    "    Get the predicted unrated item id from user id\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    userid : int\n",
    "        The user id\n",
    "\n",
    "    estimator : Surprise object\n",
    "        The estimator\n",
    "\n",
    "    unrated_item_id : set\n",
    "        The unrated item id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pred_data : pandas Dataframe\n",
    "        The predicted rating of unrated item of user id\n",
    "    \"\"\"\n",
    "    # Initialize dict\n",
    "    pred_dict = {\n",
    "        'user_id': userid,\n",
    "        'item_id': [],\n",
    "        'predicted_rating': []\n",
    "    }\n",
    "\n",
    "    # Loop for over all unrated movie Id\n",
    "    for id in unrated_item_id:\n",
    "        # Create a prediction\n",
    "        pred_id = estimator.predict(uid = pred_dict['user_id'],\n",
    "                                    iid = id)\n",
    "\n",
    "        # Append\n",
    "        pred_dict['item_id'].append(id)\n",
    "        pred_dict['predicted_rating'].append(pred_id.est)\n",
    "\n",
    "    # Create a dataframe\n",
    "    pred_data = pd.DataFrame(pred_dict).sort_values('predicted_rating',\n",
    "                                                     ascending = False)\n",
    "\n",
    "    return pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>9</td>\n",
       "      <td>2026</td>\n",
       "      <td>4.536968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>9</td>\n",
       "      <td>1498</td>\n",
       "      <td>4.530275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>920</th>\n",
       "      <td>9</td>\n",
       "      <td>924</td>\n",
       "      <td>4.530151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190</th>\n",
       "      <td>9</td>\n",
       "      <td>2195</td>\n",
       "      <td>4.520749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>9</td>\n",
       "      <td>208</td>\n",
       "      <td>4.513122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>696</th>\n",
       "      <td>9</td>\n",
       "      <td>700</td>\n",
       "      <td>3.807598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1174</th>\n",
       "      <td>9</td>\n",
       "      <td>1178</td>\n",
       "      <td>3.801894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>9</td>\n",
       "      <td>2065</td>\n",
       "      <td>3.796135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>9</td>\n",
       "      <td>35</td>\n",
       "      <td>3.716102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>9</td>\n",
       "      <td>288</td>\n",
       "      <td>3.701496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2264 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  predicted_rating\n",
       "2021        9     2026          4.536968\n",
       "1494        9     1498          4.530275\n",
       "920         9      924          4.530151\n",
       "2190        9     2195          4.520749\n",
       "205         9      208          4.513122\n",
       "...       ...      ...               ...\n",
       "696         9      700          3.807598\n",
       "1174        9     1178          3.801894\n",
       "2060        9     2065          3.796135\n",
       "32          9       35          3.716102\n",
       "285         9      288          3.701496\n",
       "\n",
       "[2264 rows x 3 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_unrated_movie = get_pred_unrated_item(userid = 9,\n",
    "                                                estimator = model_best,\n",
    "                                                unrated_item_id = unrated_item_id)\n",
    "\n",
    "predicted_unrated_movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get highest unrated data\n",
    "def get_top_highest_unrated(estimator, k, userid, trip_data):\n",
    "    \"\"\"\n",
    "    Get top k highest of unrated movie from a Surprise estimator RecSys\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : Surprise model\n",
    "        The RecSys model\n",
    "\n",
    "    k : int\n",
    "        The number of Recommendations\n",
    "\n",
    "    userid : int\n",
    "        The user Id to recommend\n",
    "\n",
    "    trip_data : pandas Data Frame\n",
    "        The rtrip data\n",
    "\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    top_item_pred : pandas DataFrame\n",
    "        The top items recommendations\n",
    "    \"\"\"\n",
    "    # 1. Get the unrated item id of a user id\n",
    "    unrated_item_id = get_unrated_item(userid=userid, trip_data=trip_data)\n",
    "\n",
    "    # 2. Create prediction from estimator to all unrated item id\n",
    "    predicted_unrated_item = get_pred_unrated_item(userid = userid,\n",
    "                                                   estimator = estimator,\n",
    "                                                   unrated_item_id = unrated_item_id)\n",
    "\n",
    "    # 3. Sort & add meta data\n",
    "    top_item_pred = predicted_unrated_item.head(k).copy()\n",
    "\n",
    "    return top_item_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>100</td>\n",
       "      <td>731</td>\n",
       "      <td>4.491742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1385</th>\n",
       "      <td>100</td>\n",
       "      <td>1390</td>\n",
       "      <td>4.487410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>100</td>\n",
       "      <td>372</td>\n",
       "      <td>4.462023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>100</td>\n",
       "      <td>740</td>\n",
       "      <td>4.460376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>100</td>\n",
       "      <td>1022</td>\n",
       "      <td>4.454190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>100</td>\n",
       "      <td>2241</td>\n",
       "      <td>4.442711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>100</td>\n",
       "      <td>686</td>\n",
       "      <td>4.426250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090</th>\n",
       "      <td>100</td>\n",
       "      <td>1095</td>\n",
       "      <td>4.417950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>100</td>\n",
       "      <td>73</td>\n",
       "      <td>4.415896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2163</th>\n",
       "      <td>100</td>\n",
       "      <td>2168</td>\n",
       "      <td>4.408642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  predicted_rating\n",
       "726       100      731          4.491742\n",
       "1385      100     1390          4.487410\n",
       "367       100      372          4.462023\n",
       "735       100      740          4.460376\n",
       "1017      100     1022          4.454190\n",
       "2236      100     2241          4.442711\n",
       "681       100      686          4.426250\n",
       "1090      100     1095          4.417950\n",
       "73        100       73          4.415896\n",
       "2163      100     2168          4.408642"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 10 recommendation for user 100\n",
    "get_top_highest_unrated(estimator=model_best,\n",
    "                        k=10,\n",
    "                        userid=100,\n",
    "                        trip_data=trip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>predicted_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1803</th>\n",
       "      <td>500</td>\n",
       "      <td>1808</td>\n",
       "      <td>4.528961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>500</td>\n",
       "      <td>1130</td>\n",
       "      <td>4.465464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>500</td>\n",
       "      <td>2199</td>\n",
       "      <td>4.455162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1742</th>\n",
       "      <td>500</td>\n",
       "      <td>1747</td>\n",
       "      <td>4.445118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>500</td>\n",
       "      <td>911</td>\n",
       "      <td>4.443593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1110</th>\n",
       "      <td>500</td>\n",
       "      <td>1115</td>\n",
       "      <td>4.437241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>500</td>\n",
       "      <td>491</td>\n",
       "      <td>4.421853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>500</td>\n",
       "      <td>295</td>\n",
       "      <td>4.420786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>500</td>\n",
       "      <td>897</td>\n",
       "      <td>4.418841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>500</td>\n",
       "      <td>383</td>\n",
       "      <td>4.418658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id  item_id  predicted_rating\n",
       "1803      500     1808          4.528961\n",
       "1125      500     1130          4.465464\n",
       "2194      500     2199          4.455162\n",
       "1742      500     1747          4.445118\n",
       "906       500      911          4.443593\n",
       "1110      500     1115          4.437241\n",
       "487       500      491          4.421853\n",
       "292       500      295          4.420786\n",
       "892       500      897          4.418841\n",
       "380       500      383          4.418658"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 10 recommendation for user 500\n",
    "get_top_highest_unrated(estimator=model_best,\n",
    "                        k=10,\n",
    "                        userid=500,\n",
    "                        trip_data=trip_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
